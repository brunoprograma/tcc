{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testes_lda_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ribeiroti/tcc/blob/master/testes_lda_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KETBE6jzXOxw",
        "colab_type": "text"
      },
      "source": [
        "#Implementação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0KqOXrSTSA1",
        "colab_type": "text"
      },
      "source": [
        "## Dados dos datasets\n",
        "\n",
        "Dataset | Query | \\|Dataset\\| | \\|Rel\\| | % Rel\n",
        "--- | --- | --- | --- | ---\n",
        "Hall | Defect prediction | 8911 | 104 | 1.17\n",
        "Radjenovic | Defect prediction metrics | 6000 | 48 | 0.80\n",
        "Kitchenham | Literature review | 1704 | 45 | 2.64\n",
        "Wahono | Defect prediction | 7002 | 62 | 0.88"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj3DFLWBfZgK",
        "colab_type": "text"
      },
      "source": [
        "## Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-dXiNo5Y8-S",
        "colab_type": "code",
        "outputId": "1849dd17-e17d-44f4-8103-cc8d6424341b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "import sys\n",
        "import datetime\n",
        "from collections import OrderedDict\n",
        "from random import seed\n",
        "from random import randint\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Python version\")\n",
        "print (sys.version)\n",
        "print(\"Gensim version\")\n",
        "print (gensim.__version__)\n",
        "print(\"Pandas version\")\n",
        "print (pd.__version__)\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version\n",
            "3.6.9 (default, Apr 18 2020, 01:56:04) \n",
            "[GCC 8.4.0]\n",
            "Gensim version\n",
            "3.6.0\n",
            "Pandas version\n",
            "1.0.3\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW09E2WTWYeC",
        "colab_type": "text"
      },
      "source": [
        "##Números randômicos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoJ7JNnEXIMZ",
        "colab_type": "code",
        "outputId": "45add29b-83cc-4b52-ccce-af48d59a3ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# seed(datetime.datetime.now().second)\n",
        "\n",
        "random_numbers = [534, 424, 826, 310, 983]\n",
        "\n",
        "# for _ in range(5):\n",
        "# \tvalue = randint(0, 1000)\n",
        "# \trandom_numbers.append(value)\n",
        " \n",
        "print(random_numbers)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[534, 424, 826, 310, 983]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJfURIvSgKUb",
        "colab_type": "text"
      },
      "source": [
        "## Tratamento do dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP_g5WxICVS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus_for_docs(docs):\n",
        "    \"\"\"\n",
        "    Faz o tratamento da base e retorna o corpus, dicionário e query tratada\n",
        "    \"\"\"\n",
        "    # Download and import stopwords library\n",
        "    stopwds = stopwords.words('english')\n",
        "\n",
        "    # Split the documents into tokens.\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    for idx in range(len(docs)):\n",
        "        docs[idx] = str(docs[idx]).lower()  # Convert to lowercase.\n",
        "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
        "\n",
        "    # Remove stopwords.\n",
        "    docs = [[token for token in doc if token not in stopwds] for doc in docs]\n",
        "\n",
        "    # Remove rare and common tokens.\n",
        "    # Create a dictionary representation of the documents.\n",
        "    dictionary = Dictionary(docs)\n",
        "\n",
        "    # Bag-of-words representation of the documents.\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "    return corpus, dictionary, docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwYDsCa6kbCu",
        "colab_type": "text"
      },
      "source": [
        "## Executa LDA e retorna TOP documentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN6AK9bOkbjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_documents(df, corpus, dictionary, query, num_topics, random_state, top_n):\n",
        "    \"\"\"\n",
        "    Retorna TOP 10 documentos da query.\n",
        "    \n",
        "    Recebe a base original, corpus, dicionário, query de busca, número de\n",
        "    tópicos e a semente do random.\n",
        "    \"\"\"\n",
        "    # Make a index to word dictionary.\n",
        "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "    id2word = dictionary.id2token\n",
        "\n",
        "    # Train LDA model.\n",
        "    model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=id2word,\n",
        "        alpha='auto',\n",
        "        eta='auto',\n",
        "        num_topics=num_topics,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # top_topics = model.top_topics(corpus, topn=10)\n",
        "\n",
        "    # Topic coherence\n",
        "    texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
        "    cm = CoherenceModel(model=model, corpus=corpus, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence = cm.get_coherence()  # get coherence value\n",
        "\n",
        "    # query topics sorted by score\n",
        "    initial_query_topics = model.get_document_topics(dictionary.doc2bow(query))\n",
        "    query_topics = list(sorted(initial_query_topics, key=lambda x: x[1], reverse=True))\n",
        "    if top_n:\n",
        "        query_topics = query_topics[:top_n]\n",
        "\n",
        "    # Get documents with highest probability for each topic. Source: https://stackoverflow.com/a/56814624\n",
        "\n",
        "    # matrix with the query topics as columns and docs as rows, value is the proba\n",
        "    topic_ids = [x[0] for x in query_topics]\n",
        "    topic_matrix = pd.DataFrame(columns=topic_ids)\n",
        "\n",
        "    # Loop over all the documents to group the probability of each topic\n",
        "    for docID in range(len(corpus)):\n",
        "        topic_matrix.loc[len(topic_matrix)] = 0  # fill with zeros\n",
        "        topic_vector = OrderedDict(model[corpus[docID]])  # convert list of tuples to OrderedDict to go faster\n",
        "        for topicID in topic_ids:  # only the query topics are relevant\n",
        "            topic_matrix.at[docID, topicID] = topic_vector.get(topicID, 0)\n",
        "\n",
        "    # sum probas\n",
        "    # ids dos docs\n",
        "    docs = np.array((range(len(corpus))))\n",
        "    # soma de todas as colunas p/ cada documento\n",
        "    topic_probas = topic_matrix.sum(axis=1)\n",
        "    # sorteia da maior p/ a menor proba\n",
        "    ids = docs[np.argsort(topic_probas[docs])[::-1]]\n",
        "    # top_docs = topic_probas[ids]\n",
        "\n",
        "    # get TOP documents for TOP 1 or 2 query topics\n",
        "    num_relevant = 0\n",
        "    top_n_topics = len(query_topics)\n",
        "\n",
        "    # quantos documentos tenho que revisar até achar o primeiro relevante\n",
        "    counter = 0\n",
        "    f_distance = 0\n",
        "    top_10_true = 0\n",
        "    top_20_true = 0\n",
        "\n",
        "    for idx, doc in enumerate(ids):\n",
        "        counter += 1\n",
        "        doc_data = df.iloc[doc, :]\n",
        "        if doc_data['label'] == 'yes':\n",
        "            num_relevant += 1\n",
        "            if idx < 10:\n",
        "                top_10_true += 1\n",
        "            if idx < 20:\n",
        "                top_20_true += 1\n",
        "            if not f_distance:\n",
        "                f_distance = counter\n",
        "            counter = 0\n",
        "\n",
        "    return len(initial_query_topics), top_n_topics, coherence, len(ids), num_relevant, f_distance, top_10_true, top_20_true "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVXberDcktet",
        "colab_type": "text"
      },
      "source": [
        "## Combinações de teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ASUZ1tQYMda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dados de teste: DBs, número de tópicos e sementes do random state,\n",
        "DATABASES = {'Hall': 'Defect prediction', 'Radjenovic': 'Defect prediction metrics', 'Kitchenham': 'Literature review', 'Wahono': 'Defect prediction'}\n",
        "NUM_TOPICS_FOR_TEST = range(10, 201, 10)\n",
        "FIXED_OPTIONS = [0, 1]\n",
        "\n",
        "# dicionário onde serão armazenados os resultados\n",
        "results = {\n",
        "    'database': [],                 # nome da base\n",
        "    'query': [],                    # query\n",
        "    'num_topics': [],               # número de tópicos utilizado\n",
        "    'qry_topics': [],               # número de tópicos retornado para a query\n",
        "    'top_n_topics': [],             # número de tópicos usados para reunir os documentos\n",
        "    'fixed_n_topics': [],           # número de tópicos foi fixado? falso para cálculo proporcional\n",
        "    'num_documents': [],            # número total de documentos\n",
        "    'rand_seed': [],                # semente do random do modelo LDA\n",
        "    'coherence': [],                # coerência dos tópicos com a coleção\n",
        "    'num_relevant': [],             # número de documentos relevantes encontrados no TOP 10\n",
        "    'f_distance': [],               # distância até o primeiro relevante\n",
        "    'top_10_true': [],              # documentos relevantes no top 10\n",
        "    'top_20_true': [],              # documentos relevantes no top 20\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FQS9NUYYi3",
        "colab_type": "text"
      },
      "source": [
        "#Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCkxTatmOkyh",
        "colab_type": "code",
        "outputId": "815cd5f1-ccdd-4252-d648-7c7582707f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "for dbname, query in DATABASES.items():\n",
        "    path = 'https://github.com/fastread/src/raw/master/workspace/data/{}.csv'.format(dbname)\n",
        "\n",
        "    #Carregando dataset\n",
        "    df = pd.read_csv(path, header=0, encoding='latin-1')\n",
        "\n",
        "    N_DOCS = df.shape[0]  # ilimited n_docs\n",
        "\n",
        "    base_docs = df['Document Title'] + ' ' + df['Abstract']\n",
        "\n",
        "    corpus, dictionary, docs = get_corpus_for_docs(base_docs)\n",
        "    tmp_corpus, tmp_dict, tmp_query = get_corpus_for_docs([query]) # query tem o mesmo preprocessamento da coleção\n",
        "    query = tmp_query[0]\n",
        "\n",
        "    header = ' | '.join([key.ljust(len(' '.join(query))) for key in results.keys()])\n",
        "    print(header)\n",
        "    print('='*len(header))\n",
        "\n",
        "    for fixed in FIXED_OPTIONS:\n",
        "        for rand_seed in random_numbers:\n",
        "            for num_topics in NUM_TOPICS_FOR_TEST:\n",
        "                qry_topics, top_n_topics, coherence, num_documents, num_relevant, f_distance, top_10_true, top_20_true = get_top_documents(df, corpus, dictionary, query, num_topics, rand_seed, fixed)\n",
        "                values = [dbname, ' '.join(query), str(num_topics), str(qry_topics), str(top_n_topics), str(fixed), str(num_documents), str(rand_seed), '%.6f' % coherence, str(num_relevant), str(f_distance), str(top_10_true), str(top_20_true)]\n",
        "                print(' | '.join([value.ljust(len(values[1])) for value in values]))\n",
        "                results['database'].append(dbname)\n",
        "                results['query'].append(' '.join(query))\n",
        "                results['num_topics'].append(num_topics)\n",
        "                results['qry_topics'].append(qry_topics)\n",
        "                results['top_n_topics'].append(top_n_topics)\n",
        "                results['fixed_n_topics'].append(fixed)\n",
        "                results['num_documents'].append(num_documents)\n",
        "                results['rand_seed'].append(rand_seed)\n",
        "                results['coherence'].append(coherence)\n",
        "                results['num_relevant'].append(num_relevant)\n",
        "                results['f_distance'].append(f_distance)\n",
        "                results['top_10_true'].append(top_10_true)\n",
        "                results['top_20_true'].append(top_20_true)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "database          | query             | num_topics        | qry_topics        | top_n_topics      | fixed_n_topics    | num_documents     | rand_seed         | coherence         | num_relevant      | f_distance        | top_10_true       | top_20_true      \n",
            "=================================================================================================================================================================================================================================================================\n",
            "Hall              | defect prediction | 10                | 10                | 10                | 0                 | 8911              | 534               | 0.463760          | 104               | 217               | 0                 | 0                \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-097c77061873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrand_seed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_numbers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mNUM_TOPICS_FOR_TEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mqry_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_relevant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_10_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_20_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdbname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqry_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_n_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%.6f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_relevant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_10_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_20_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' | '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c67d3b318caa>\u001b[0m in \u001b[0;36mget_top_documents\u001b[0;34m(df, corpus, dictionary, query, num_topics, random_state, top_n)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    975\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reached the end of input; now waiting for all remaining jobs to finish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mother\u001b[0m  \u001b[0;31m# frees up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_mstep\u001b[0;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_eta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_eta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextra_pass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate_eta\u001b[0;34m(self, lambdat, rho)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \"\"\"\n\u001b[1;32m    773\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mlogphat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambdat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlogphat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \"\"\"\n\u001b[1;32m    773\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mlogphat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambdat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlogphat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWT1a0NQc35e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('drive', force_remount=True)\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results.to_csv('drive/My Drive/Pureza Tópicos/resultados_lda_final.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}